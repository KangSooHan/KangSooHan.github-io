<h2 id="camera">Camera</h2>
<p>이번장에서는 카메라에 대해서 학습하도록 하겠습니다. <br />
카메라는 2D Image 정보를 해석할 수 있습니다. 이는 인간의 눈과 같은 단점을 가지기도 합니다.(ex Darkness, 빛번짐, 많은 비…) 하지만 이러한 점은 사람의 Visual Perception을 기반으로 구성된 우리 환경에서 강점으로 다가올 수 있습니다.(ex 신호등은 빨간 주황 초록 …). 따라서 우리는 카메라에 대해서 더 나아가 이미지를 처리하는 기술들(Image Processing &amp; Computer Vision)</p>
<ul>
  <li>카메라 기술 &amp; 광학</li>
  <li>Image Processing &amp; Computer Vision</li>
  <li>Sensur Fusion with Lidar</li>
  <li>Project : Collision Avoidance</li>
</ul>

<p>에 대해서 학습하도록 하겠습니다.</p>

<p><br /></p>
<h3 id="camera--rader--lidar">Camera &amp; Rader &amp; Lidar</h3>
<p>Sensor Fusion을 하는 이유는 각각 센서들이 가지고 있는 장점들이 다르기 때문입니다. 표는 다음과 같이 각각의 센서들이 좋은 점과 나쁜 점을 알 수 있습니다.</p>
<p><img src="/assets/img/sensorfusion/SensorTable.jpg" /></p>

<p>Camera Technology 
카메라 기술은 이미지를 3D Space로 재구성 할 때 많은 도움이 되며, 카메라 geometry와 coordinate system을 배우는 것이 중요합니다. 따라서 이번 장에서는 Camera Basic Technology(pinhole camera, lenses …)에 대해서 학습합니다.</p>

<p><br /></p>
<h3 id="pinhole-camera">Pinhole Camera</h3>
<p>간단한 카메라는 Ojbect 사이에 작은 구멍(Pinhole)이 있는 빛 장벽을 배치하여 설계할 수 있습니다. 물체에 의해 반사된 빛은 핀홀을 통과하여 이미지로 저장하는 감광성(빛에 노출됐을 때 반응하는) 센서에 도달하게 됩니다. 핀홀이 작은 이유는 Object의 여러 부분에서 발생하는 빛들이 중첩되어 이미지가 Blurring 되는 것을 방지하기 위해서입니다.</p>
<p><img src="/assets/img/sensorfusion/PinHole.jpg" width="50%" height="50%" /></p>

<p>Pinhole 카메라에서 Photosensitive Surface는 왼쪽에 Image Plane입니다. Camera center인 pinhole과 image plane까지의 수직 거리를 focal length $f$라 부릅니다. <br />
오프젝트의 한 점 포인트 $P$에서 반사된 빛을 image plane에 $P’$으로 매핑시키면 아래 그림과 같이 됩니다.</p>
<p><img src="/assets/img/sensorfusion/PointMapping.jpg" width="30%" height="30%" /></p>

<p>이 때 $P$와 $P’$의 관계는 다음과 같이 나타낼 수 있습니다. <br />
$\overrightarrow{P} = \begin{bmatrix} x \\ y \\ z \\ \end{bmatrix} \rightarrow \overrightarrow{P’} = \begin{bmatrix} x’ \\ y’ \\ \end{bmatrix}$ <br />
$(1)\;x’\,=\,f\cdot\frac{x}{z} \qquad(2)\;y’\,=\,f\cdot\frac{y}{z}$ <br />
이 수식을 사용하면 우리는 3D 좌표와 Focal Length를 사용하여 image plane에 생기는 2D 좌표를 획득할 수 있습니다. 하지만 이는 pixel 단위는 아직 아닙니다.</p>

<h4 id="pinhole-camera-problem">Pinhole Camera Problem</h4>
<p>Pinhole Camera의 가장 큰 문제점은 핀홀을 통과하는 빛의 양이 이미지 센서에 적절한 이미지를 생성하기에 충분하지 않은 것입니다. 그렇다고 핀홀의 크기를 넓혀 빛의 양을 증가시키면 대상 물체의 다른 부분에서 나오는 광선이 중첩되어 Blurring 효과가 발생합니다. 이러한 방법을 해결하는 것이 같은 위치에서 나오는 빛 광선을 포착할 수 있는 Lens를 사용하는 것입니다.</p>
<p><img src="/assets/img/sensorfusion/LargePinhole.jpg" width="50%" height="50%" /></p>

<p><br /></p>
<h3 id="lenses-and-aperture">Lenses and Aperture</h3>
<p>적절한 크기와 위치의 렌즈는 Object Point $P_{1}$ 의 빛 광선을 굴절시켜 image plane의 $ P^{‘}_{1}$ 지점으로 모아줍니다.</p>

<p>Object Point $ P $의 거리가 더 가깝거나( $P_{2}$ ), 멀리 있는 경우($P_{1}$) image plane에서 point로 나타나지 않습니다. $P_{2}$의 경우 광선의 집합이 이미지 평면에 초점이 맞지 않기 때문에 유한한 반지름을 가진 원에 수렴하게 됩니다.($P_{1}$의 경우도 유사) 이 Blurring한 원을 우리는 COF(circle of confusion)이라 부릅니다. 우리는 조리개(Aperture)를 조절하여 이러한 blurring circle을 줄일 수 있습니다.</p>

<p><img src="/assets/img/sensorfusion/Aperture.jpg" width="30%" height="30%" /></p>

<ul>
  <li>조리개를 작게 줄이면 <br />
렌즈의 바깥부분을 통해 들어오는 빛이 차단되면서, image plane에 맺히는 COF의 크기가 줄어들게 됩니다. 그러나 적은 빛이 들어오기 때문에 이미지가 어둡게 생성됩니다.</li>
  <li>조리개를 크게 만들면 <br />
더 많은 빛이 이미지 영역에 들어오면서 더 밝은 이미지를 생성합니다. 하지만 COF의 크기가 커지게 됩니다.</li>
</ul>

<p><img src="/assets/img/sensorfusion/Aperture_Image.jpg" width="30%" height="30%" /></p>

<p>사진에서는 F값(조리개 값)이 높아 질수록 렌즈 조리개 구멍이 작아집니다.(16일 때 가장 어두움)</p>

<p><br /></p>
<h3 id="카메라-왜곡">카메라 왜곡</h3>
<p>여기까지 우리는 3D 물체가 PinHole Camera에서 상을 맺히는 것을 배웠습니다. 카메라에서도 3D 물체가 2D Image 상이 맺힐 때 유사한 과정을 거치지만, 실제로는 렌즈는 렌즈 유형에 따라 이미지에 왜곡을 발생시킵니다. 우리는 이런 왜곡을 “Radial Distortion”이라 부릅니다. 이러한 현상은 렌즈와 image plane까지의 focal length가 직경에 걸처 “균일”하지 않기 때문에 발생합니다. 따라서 카메라 중심(광축)과 렌즈를 통과하는 광선 사이의 거리에 따라서 렌즈의 확대 효과가 달라지게 됩니다. 우리는 배율이 증가해서 나오는 왜곡 효과를 “Pin Cushion Distortion”, 배율이 감소해서 나오는 왜곡 효과를 “Barrel Distortion”이라고 합니다. 광각 렌즈를 사용할 때 일반적으로는 Barrel Distortion이 발생합니다.</p>
<p><img src="/assets/img/sensorfusion/Distortion.jpg" width="30%" height="30%" /></p>

<p>이미지에서 Object의 정보를 얻기 위해서 왜곡 정보는 제거되거나 완화되어야 합니다. 이러한 과정을 우리는 Calibration이라고 부릅니다. 이 과정은 카메라 렌즈 설정에 대해 Distortion Parameters를 개별적으로 계산할 수 있는 과정을 의미합니다. 이는 일반적으로 알려진 Planar Checkerboard 사진을 여러장 촬영함으로써 수행할 수 있습니다. Planar Checkerboard는 기하학적 형상에서 렌즈와 이미지 센서 매개변수를 강력하게 도출할 수 있는 형태의 보드로 아래 그림과 같은 모양입니다. 우리는 이 Planar Checkerboard를 사용하여 카메라 이미지에서 왜곡을 제거하는데 이 과정을 Rectification(정류)라 부릅니다.</p>
<p><img src="/assets/img/sensorfusion/Checker_Board.jpg" width="30%" height="30%" /></p>

<h3 id="projection-of-points-in-3d-space-onto-the-digital-image">Projection of points in 3D Space onto the digital Image</h3>
<p>앞서 말한바와 같이 3D Space의 점을 Image Plane에 Projection(투영)한 결과는 pixel로 이루어진 digital images에 완벽하게 일치하지 않습니다. 이번에는 Continuous한 결과를 Discrete한 pixel로 표현하는 digital 이미지에 대해 이해해 보도록 하겠습니다.(아래 그림 참조)</p>
<p><img src="/assets/img/sensorfusion/Digital_Image.jpg" width="50%" height="50%" /></p>

<h4 id="coordinate-system으로-변환">Coordinate System으로 변환</h4>
<p>Camera Center Position $O$는 axes $i, j, k$로 표현할 수 있으며, $k$는 image plane으로의 방향을 나타냅니다. Position $C^{‘}$은 k가 image plane과 교차하는 지점으로 이미지 좌표의 중심을 나타내는 principal point 혹은 center point입니다. 우선 Point $P$를 image plane에 projection시키기 위해 center point $O$에서 Point $P$를 빼서 image plane에 맺히는 Point $\overrightarrow{P}$를 계산할 수 있습니다.(image plane의 좌측 코너) 이 때, image plane에 맺히는 $\overrightarrow{P}$는 $\begin{bmatrix} P_{x} &amp; P_{y} &amp; P_{z} \end{bmatrix}$  로 표현할 수 있습니다. 다음은 transformation process로 미터로 계산한 $\overrightarrow{P}$를 pixel 좌표로 이동시키는 것입니다. 이 때 우리는 아래 투영 방정식을 통해 얻을 수 있는 $k$와 $l$ 파라미터를 사용하여 meters를 pixel로 변환할 수 있습니다.($l, k$는 추후에 mapping operation을 할 때 필요한 calibration matrix에 중요하게 사용됩니다.) 이 때, y좌표가 좌측 하단에 있는 것이 아닌 좌측 상단에 있는것을 꼭 기억해야 합니다. <br />
$ (1)\qquad \overrightarrow{P} \rightarrow \overrightarrow{P^{‘}}$ <br />
$ (2)\qquad \begin{pmatrix} x, &amp; y, &amp; z \end{pmatrix}^{T} \rightarrow ( \underbrace{f \cdot k}_{\alpha} \cdot \frac{x}{z} +c_x$, $\underbrace{f \cdot l}_{\beta}\cdot\frac{y}{z} + c_{y} )^{T}$ <br />
$ [k,l] = pixels/m $ \</p>

<p>일반적으로 이와 같은 rectification으로 m를 픽셀로 변환할 때, 정확하게 discrete한 pixel에 일치하지 않습니다. 이러한 점을 보완하기 위해 interpolation을 사용하는데 이러한 interpolation errors를 피하기 위해서 원본 이미지를 사용하는 것은 합리적입니다. feature tracking과 같은 task에서 origin image를 사용하고 rectification을 적용하는 것이 의미 있습니다. 하지만 deep learning을 사용하는 경우는 distortion이 detection error를 가져올 수 있습니다.</p>

<p><br /></p>
<h3 id="image-and-bayer-pattern">Image and Bayer Pattern</h3>
<p>이번장에서는 빛 광선이 어떤 파장을 가지고 digitally하게 저장되는 Color Pixel로 변환되는지에 대해서 배우도록 하겠습니다. <br />
Camera로 이미지를 촬영하면, 빛이 렌즈를 통과하여 이미지 센서에 도달합니다. 이 센서는 얼마나 많은 빛이 들어오는지 등록하는 light sensitive elements로 구성되어있고 그것을 상응하는 electron(..?)의 수로 변경해줍니다.(More Light, More Electron) 일정 노출 시간(Exposure time)이 완료되면, 이 생성된 전자들은 전압으로 변환되며, 최종적으로 A/D-Converter를 통해 이산 숫자로 변환됩니다. 이미지 기술에는 현재 CCD, CMOS 2가지 기술이 주로 사용되고 있습니다. 두 기술 모두 전자를 Voltage(전압)으로 변경시켜주지만 다른 전자를 생성하는 파장을 구분하지 못하는 색맹입니다. 그래서 Color Vision을 하기 위해서 Pixel앞에 특정 파장만을 허용하는 작은 filter elements(also micro-lenses)를 사용합니다. 파장을 색상에 매핑시키는 가장 일반적인 방법은 기본 원색이 개별적으로 통과할 수 있도록 RGB 패턴으로 필터 요소를 배열 하는 것입니다. 이러한 RGB 값들의 색 조합을 사용하면 사람이 볼 수 있는 대부분의 색깔을 만들어 낼 수 있습니다. 또 각 색상을 8비트(0~255까지 256개의 값)로 코딩하면 256<em>256</em>256=1670만 가지 색상을 생성할 수 있습니다. \</p>

<p><br /></p>
<h4 id="bayer-pattern">Bayer Pattern</h4>
<p>가장 보편적으로 RGB 패턴을 정렬시키는 방법을 Bayer Pattern이라 부릅니다. 만약 2500 x 2000 = 5백만 화소의 이미지를 가지고 있다면 R, G, B 색을 감지할 수 있는 화소가 5백만개 포함된 이미지 센서를 사용하는 것입니다. 그런데 이 때 각 화소가 RGB를 파악하는 것이 아닌 흑백의 밝기만을 감지하는 monochrome 화소이면 데이터 전송량을 엄청나게 줄일 수 있습니다. 이 때, 인간의 시각 특성에 따라서 G가 50% R, B가 25씩 되도록 아래 그림과 같이 교차 배치하는 방식을 Bayer Pattern이라 합니다.</p>
<p align="center"><img src="/assets/img/sensorfusion/Bayer_Pattern_Filter.jpg" width="70%" height="70%" /></p>

<p><br /></p>
<h4 id="ccdcharge-coupled-device">CCD(Charge-Coupled Device)</h4>
<p>CCD 센서는 각 그림에서 수집된 전자들이 단일 또는 소수의 출력 노드를 통해 칩에서 전송됩니다. 그 이후 전하가 전압으로 변경되고 버퍼링 되어 아날로그 신호로 전송됩니다. 그 이후 신호가 증폭되고 센서 외부의 A/D 변환기를 통해 discrete한 숫자로 변환됩니다. CCD 센서는 CMOS에 비해 높은 감광도와 적은 노이즈를 가지고 있었지만 오늘날은 그 차이가 많이 줄어 들었습니다. 반면에 CCD는 높은 생산 가격과 높은 전력소비를 하는 단점을 가지고 있습니다.</p>

<p><br /></p>
<h4 id="cmoscomplementary-metal-oxide-semiconductor">CMOS(Complementary Metal-oxide Semiconductor)</h4>
<p>CMOS 센서는 반도체 소자를 이용해 빛의 세기를 측정합니다. 반도체라는 것은 전압을 걸지 않으면 부도체, 전압을 걸면 전기가 흐르는 도체가 되는데 이를 사용하여 측정값을 한번에 병렬로 읽는 방식입니다. 또한 병렬로 읽기 때문에 CCD와 달리 빛의 세기를 측정하는 회로 장치가 셀 마다 전부 존재합니다. 이러한 특징 때문에 소비 전력이 작고, 고속 처리가 가능한 장점을 가지고 있습니다. 단점은 CMOS센서는 회로가 가지고 있는 (재질적, 공정적, 물리적)편차를 가지고 있기 때문에 전위를 측정하는 방식이 동일하지 않기 때문에 낮은 감도를 가지고 있습니다. 또한 암 전류가 잔존하여 노이즈가 많이 발생할 수 있는 단점을 가지고 있습니다. 하지만 오늘날 기술들이 비약적으로 발전하면서 이러한 문제를 많이 해결하였고 요즘은 CMOS를 대부분 사용하는 추세입니다.</p>

<p align="center"><iframe width="720" height="405" src="https://www.youtube.com/embed/nxUDHcZl1uo" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe></p>

<p><br /><br /><br /><br /><br /></p>

<hr />
<h3 id="단어">단어</h3>
<p>Gleaned : 얻은, 수집된 <br />
Glean : 얻다 수집하다 <br />
Embark : 배에 승선하다 <br />
Embork : 시동을 걸다 <br />
Contraption : 기구 장치 <br />
Matured : 다 큰, 분별 있는 <br />
Profoundly : 깊이 <br />
Adverse : 불리한 부정적인 <br />
Preceding : 선행의 <br />
Fierce : 사나운 <br />
Debate : 토론 논쟁 <br />
Unattractive : 매력 없는 <br />
Emanating : 방출 중 <br />
Superimposed : 겹친 <br />
Aperture : 구멍 <br />
Photosensitive : 감광성 <br />
Decent : 적절한 <br />
Emanate : 발하다 <br />
Aperture : 구멍, 개구부 <br />
Refract : 굴절시키다 <br />
Such that : 그런식으로 <br />
Concentric : 중심이 같은 <br />
Magnification : 확대 <br />
Mitigate : 완화하다 <br />
Along with : ~와 함께 <br />
Along : ~를 따라 <br />
Advisible : 권고할만한, 가치있는 <br />
Electrons : 전자</p>
